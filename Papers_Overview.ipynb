{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Papers\n",
    "\n",
    "In this notebook we will get into 4 (four) main papers about **Dynaminc Vocabulary**, **Transfer Learning** and **Neural Machine Translation** that I have selected from differents places.\n",
    "\n",
    "For each paper I divide in 5 (five) topics of explanation, (**Goals**, **Approach**, **Experiments**, **Results** and **Thoughts**). \n",
    "\n",
    "\n",
    "![Image](media/C69DKaQXAAAhUpb.jpg)\n",
    "\n",
    "Above is a list the papers that we will review here, but there is a list with more papers in this repository you can find in **RESOURCES-ENG.md**. The list of papers is always growing. \n",
    "\n",
    "\n",
    "|Ano|Titulo|Autor|Link|\n",
    "|----------------|-------------------------------|----------------|-------------------------------|\n",
    "|2019|Hierarchical Transfer Learning Architecture for Low-Resource Neural Machine Translation|Gongxu Luo, et al|[`PDF`](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8805098)|\n",
    "|2018|Transfer Learning in Multilingual Neural Machine Translation with Dynamic Vocabulary|Surafel M. Lakew, et al.|[`PDF`](https://arxiv.org/pdf/1811.01137.pdf)|\n",
    "|2018|Incorporating Statistical Machine Translation Word Knowledge into Neural Machine Translation|Xing Wang, et al.|[`PDF`](https://ieeexplore.ieee.org/document/8421063)|\n",
    "|2017|Neural Response Generation with Dynamic Vocabularies|Yu Wu, et al.|[`PDF`](https://arxiv.org/pdf/1711.11191.pdf)|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. How it works"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Hierarchical Transfer Learning Architecture for Low-Resource Neural Machine Translation\n",
    "\n",
    "### Abstract\n",
    "\n",
    "We propose a method to **transfer knowledge** across neural machine translation (NMT) models by means of a shared **dynamic vocabulary**. Our approach allows to extend an initial model for a given language pair to cover new languages by **adapting its vocabulary as long as new data become available** (i.e., introducing new vocabulary items if they are not included in the initial model). The parameter transfer mechanism is evaluated in two scenarios: i) to adapt a trained single language NMT system to work with a new language pair and ii) to continuously add new language pairs to grow to a multilingual NMT system. In both the scenarios our goal is to\n",
    "improve the translation performance, while minimizing the training convergence time. Preliminary experiments spanning five languages with different training data sizes (i.e., 5k and 50k parallel sentences) show a significant performance **gain ranging from +3.85 up to +13.63 BLEU** in different language directions. Moreover, when compared with training an NMT model from scratch, **our transfer-learning approach** allows us to reach higher performance after training up to 4% of the total training steps.\n",
    "\n",
    "### 2.1. Papers Goals\n",
    "\n",
    "Explore transfer-learning technique in **Multilingual Neural Machine Translation** using dynamic vocabularies (e.g German to English, Italy to English).\n",
    "\n",
    "![Image](media/MNTL_Diagram.png)\n",
    "\n",
    "The idea is work like Google Translation but of course with less vocabulary.\n",
    "\n",
    "### 2.2. Approach\n",
    "\n",
    "\n",
    "![Image](media/Approach.png)\n",
    "\n",
    "• *progAdapt*, in which progressive updates are made on the assumption that new target NMT task data become available for one language direction at a time (i.e., new language directions are covered sequentially). In this condition, our goal is to maximize performance on the new target tasks by taking advantage of parameters learned in their parent task;\n",
    "\n",
    "• *progGrow*, in which progressive updates are made on the same assumption of receiving new target task\n",
    "data as in progAdapt, but with the additional goal of preserving the performance of the previous language directions.\n",
    "\n",
    "For the **Dynamic Vocabulary** the approach simply keeps the intersection (same entries). At training time, these new entries are randomly initialized, while the intersecting items maintain the embeddings of the former model.\n",
    "\n",
    "Example, let's imagine the our vocabulary has only **2** words (*hello* and *world*). Below you can see a code how our Word Embedding should be at the first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.1427, -0.0849,  0.0206,  2.4439,  1.4003]],\n",
      "       grad_fn=<EmbeddingBackward>) tensor([[-0.4128,  0.2552, -1.4442, -0.9061, -0.3941]],\n",
      "       grad_fn=<EmbeddingBackward>)\n"
     ]
    }
   ],
   "source": [
    "word2index = {\"hello\": 0, \"world\": 1}\n",
    "embeds = nn.Embedding(2, 5) # 2 words in vocab, 5 dimensional embeddings\n",
    "hello_embed = embeds(torch.tensor([word2index[\"hello\"]], dtype=torch.long))\n",
    "world_embed = embeds(torch.tensor([word2index[\"world\"]], dtype=torch.long))\n",
    "print(hello_embed, world_embed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you want add one more word (*keyboard*), in the proposed approach, give a new word the *Embedding* should keep our weights already trained and concatenate with a new weight for *keyboard* initialized randomly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.1427, -0.0849,  0.0206,  2.4439,  1.4003]]) tensor([[-0.4128,  0.2552, -1.4442, -0.9061, -0.3941]]) tensor([[0.9518, 0.1304, 0.4702, 0.6974, 0.5702]])\n"
     ]
    }
   ],
   "source": [
    "word2index = {\"hello\": 0, \"world\": 1, \"keyboard\": 2} # updated vocabulary\n",
    "concat_embeds = torch.FloatTensor([\n",
    "    hello_embed.detach().numpy()[0], # old embed\n",
    "    world_embed.detach().numpy()[0], # old embed\n",
    "    np.random.rand(5) # new embed initialized randomly\n",
    "])\n",
    "embeds = nn.Embedding.from_pretrained(concat_embeds) # 3 words in vocab, 5 dimensional embeddings\n",
    "hello_embed = embeds(torch.tensor([word2index[\"hello\"]], dtype=torch.long))\n",
    "world_embed = embeds(torch.tensor([word2index[\"world\"]], dtype=torch.long))\n",
    "keyboard_embed = embeds(torch.tensor([word2index[\"keyboard\"]], dtype=torch.long))\n",
    "print(hello_embed, world_embed, keyboard_embed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. Experiments\n",
    "\n",
    "Their experimental setting includes the init model language pair **(German-English)** and three additional language pairs (**Italian-English**, **Romanian-English**, and **Dutch-English**) for testing the proposed approaches.\n",
    "\n",
    "The baseline models, referred to as **Bi-NMT**, are separately **trained from scratch in a bi-directional** setting (i.e., source ↔ target). In addition, we report scores from a **multilinugal (M-NMT)** model trained with the concatenation of all available data in **each training stage**.\n",
    "\n",
    "### 2.4. Results\n",
    "\n",
    "![Image](media/ResultGrowAdapted.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Incorporating Statistical Machine Translation Word Knowledge into Neural Machine Translation\n",
    "\n",
    "### Abstract\n",
    "\n",
    "**Neural Machine Translation (NMT)** has gained more and more attention in recent years, mainly due to its simplicity yet state-of-the-art performance. However, previous research has shown that NMT suffers from several limitations: source coverage guidance, **translation of rare words**, and **the limited vocabulary**, while **Statistical Machine Translation (SMT)** has complementary properties that correspond well to these limitations. It is straightforward to improve translation performance by combining the advantages of two kinds of models. This article proposes a general framework for **incorporating the SMT word knowledge into NMT** to alleviate above **word-level limitations**. In our framework, the NMT decoder makes more accurate word prediction by referring to the **SMT word recommendations** in both training and testing phases. Specifically, the **SMT model** offers informative **word recommendations** based on the **NMT decoding information**. Then we use the **SMT word predictions as prior knowledge to adjust the NMT word generation probability**, which unitizes a neural network-based classifier to digest the discrete word knowledge. In this work, we use two model variants to implement the framework, one with a **gating mechanism** and the other with a **direct competition mechanism**. Experimental results on Chinese-to-English and English-to-German translation tasks show that the proposed framework can take advantage of the SMT word knowledge and consistently achieve significant improvements over NMT and SMT baseline systems.\n",
    "\n",
    "### 4.1. Papers Goals\n",
    "\n",
    "Create general **framework** that incorporates the **SMT word knowledge into the NMT encoder-decoder architecture**. Specifically, first they train the SMT model on a parallel data by using the commonly used SMT approaches. Then they use the **SMT word knowledge as prior knowledge to adjust the NMT word generation probability**, by utilizing a neural networkbased classifier to digest the discrete word knowledge. The proposed neural network-based model makes the NMT model automatically learn to make use of the SMT knowledge, without having to heuristically combine NMT model with SMT features or SMT outputs.\n",
    "\n",
    "### 4.2. Approach\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
